{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7d48f18-43e3-40e5-b62b-cdc34feea993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from datasets import Audio\n",
    "from transformers import AutoTokenizer\n",
    "from streaming import MDSWriter\n",
    "from streaming.base.format.mds.encodings import Encoding, _encodings\n",
    "from streaming import LocalDataset\n",
    "import streaming\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import os\n",
    "import json\n",
    "from multiprocess import Pool\n",
    "import itertools\n",
    "\n",
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield (l[i: i + n], i // n)\n",
    "\n",
    "def multiprocessing(strings, function, cores=6, returned=True):\n",
    "    df_split = chunks(strings, len(strings) // cores)\n",
    "    pool = Pool(cores)\n",
    "    pooled = pool.map(function, df_split)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    if returned:\n",
    "        return list(itertools.chain(*pooled))\n",
    "\n",
    "class UInt32(Encoding):\n",
    "    def encode(self, obj) -> bytes:\n",
    "        return obj.tobytes()\n",
    "\n",
    "    def decode(self, data: bytes):\n",
    "        return np.frombuffer(data, np.uint32)\n",
    "\n",
    "_encodings['uint32'] = UInt32\n",
    "\n",
    "columns = {\n",
    "    'input_ids': 'uint32',\n",
    "    'position_ids': 'uint32',\n",
    "    'attention_mask': 'uint32',\n",
    "    'audio': 'str',\n",
    "    'text': 'str'\n",
    "}\n",
    "hashes = 'sha1', 'xxh64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e47f51eb-e6bf-4183-84ea-d1c3d6787b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2014337"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('accept-chunk-streaming-flatten.json') as fopen:\n",
    "    accepted = set(json.load(fopen))\n",
    "len(accepted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86e6a625-f97c-4fe0-a1a6-729ab82d0c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('chunk-streaming-flatten.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbba1c0a-4d4a-4918-901f-2b8d9ddf65cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename_audio</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chunk-streaming/prepare-dataset-normalizer-tex...</td>\n",
       "      <td>Menurutnya, kejadian itu dipercayai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chunk-streaming/prepare-dataset-normalizer-tex...</td>\n",
       "      <td>berlaku</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chunk-streaming/prepare-dataset-normalizer-tex...</td>\n",
       "      <td>di Kilometer tiga puluh empat lebuh raya berke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>chunk-streaming/prepare-dataset-normalizer-tex...</td>\n",
       "      <td>kelmarin.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chunk-streaming/introduction-husein-v3_97142_0...</td>\n",
       "      <td>Encik,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5327564</th>\n",
       "      <td>chunk-streaming/en_chatbot-idayu_9187_1.mp3</td>\n",
       "      <td>am afraid I do not have a physical body to dan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5327565</th>\n",
       "      <td>chunk-streaming/en_chatbot-idayu_9187_2.mp3</td>\n",
       "      <td>or music online.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5327566</th>\n",
       "      <td>chunk-streaming/response-husein-v2_156365_0.mp3</td>\n",
       "      <td>Encik, kita ada jual alat panen buah yang cekap.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5327567</th>\n",
       "      <td>chunk-streaming/response-husein-v2_156365_1.mp3</td>\n",
       "      <td>Nak tanya harga tak?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5327568</th>\n",
       "      <td>chunk-streaming/introduction-idayu-v2_126985_0...</td>\n",
       "      <td>Puan, saya dari Tropical Retreats, uhm, Puan i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5327569 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename_audio  \\\n",
       "0        chunk-streaming/prepare-dataset-normalizer-tex...   \n",
       "1        chunk-streaming/prepare-dataset-normalizer-tex...   \n",
       "2        chunk-streaming/prepare-dataset-normalizer-tex...   \n",
       "3        chunk-streaming/prepare-dataset-normalizer-tex...   \n",
       "4        chunk-streaming/introduction-husein-v3_97142_0...   \n",
       "...                                                    ...   \n",
       "5327564        chunk-streaming/en_chatbot-idayu_9187_1.mp3   \n",
       "5327565        chunk-streaming/en_chatbot-idayu_9187_2.mp3   \n",
       "5327566    chunk-streaming/response-husein-v2_156365_0.mp3   \n",
       "5327567    chunk-streaming/response-husein-v2_156365_1.mp3   \n",
       "5327568  chunk-streaming/introduction-idayu-v2_126985_0...   \n",
       "\n",
       "                                                      text  \n",
       "0                      Menurutnya, kejadian itu dipercayai  \n",
       "1                                                  berlaku  \n",
       "2        di Kilometer tiga puluh empat lebuh raya berke...  \n",
       "3                                                kelmarin.  \n",
       "4                                                   Encik,  \n",
       "...                                                    ...  \n",
       "5327564  am afraid I do not have a physical body to dan...  \n",
       "5327565                                   or music online.  \n",
       "5327566   Encik, kita ada jual alat panen buah yang cekap.  \n",
       "5327567                               Nak tanya harga tak?  \n",
       "5327568  Puan, saya dari Tropical Retreats, uhm, Puan i...  \n",
       "\n",
       "[5327569 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccf940de-1c26-4e4a-84a9-1d269457b083",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5327569/5327569 [00:54<00:00, 98367.13it/s] \n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for i in tqdm(range(len(df))):\n",
    "    if i not in accepted:\n",
    "        continue\n",
    "    speaker = 'husein' if 'husein' in df.iloc[i]['filename_audio'] else 'idayu'\n",
    "    data.append({\n",
    "        'speaker': speaker,\n",
    "        'text': df.iloc[i]['text'],\n",
    "        'token': f'chunk-streaming-flatten/{i}.json',\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "080290c0-bc21-4014-be21-9f51a3dbc164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2014337"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "695c8064-8b76-4ebb-baa2-afab2215dac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('mesolitica/Malaysian-TTS-1.7B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f0ffffc-1576-4016-8e8c-68e5748906d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def collator(batch, batch_position_ids):\n",
    "    input_ids = []\n",
    "    position_ids = []\n",
    "    masks = []\n",
    "    for i in range(len(batch)):\n",
    "        l = len(batch[i])\n",
    "        input_ids.extend(batch[i])\n",
    "        position_ids.extend(batch_position_ids[i])\n",
    "        masks.append(l)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': np.array(input_ids).astype(np.uint32),\n",
    "        'position_ids': np.array(position_ids).astype(np.uint32),\n",
    "        'attention_mask': np.array(masks).astype(np.uint32),\n",
    "        'audio': '',\n",
    "        'text': '',\n",
    "    }\n",
    "\n",
    "def slice_and_balance(nested_list, size):\n",
    "    first = []\n",
    "    balance = []\n",
    "    current_size = 0\n",
    "\n",
    "    for sublist in nested_list:\n",
    "        if current_size < size:\n",
    "            remaining_space = size - current_size\n",
    "            if len(sublist) <= remaining_space:\n",
    "                first.append(sublist)\n",
    "                current_size += len(sublist)\n",
    "            else:\n",
    "                first.append(sublist[:remaining_space])\n",
    "                balance.append(sublist[remaining_space:])\n",
    "                current_size = size\n",
    "        else:\n",
    "            balance.append(sublist)\n",
    "    \n",
    "    return first, balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb24a7bf-85dc-4e9d-a777-23ddbf44f591",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf tokenized-4k-qwen3-chunk\n",
    "!mkdir tokenized-4k-qwen3-chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "beff7079-daa2-4409-9579-6c70533131c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "sequence_length = 4096\n",
    "def loop(files, block_size = sequence_length):\n",
    "    rows, index = files\n",
    "    out_root = f'tokenized-4k-qwen3-chunk/tokenized-{index}'\n",
    "    os.system(f'rm -rf {out_root}')\n",
    "    count = 0\n",
    "    temp = []\n",
    "    position_ids = []\n",
    "    last_block, last_position_block = None, None\n",
    "    with MDSWriter(out=out_root, columns=columns, compression=None, hashes=hashes) as out:\n",
    "        for row in tqdm(rows):\n",
    "\n",
    "            speaker = row['speaker']\n",
    "            with open(row['token']) as fopen:\n",
    "                token = json.load(fopen)\n",
    "            token = ''.join([f'<|speech_{t}|>' for t in token])\n",
    "            t = row['text']\n",
    "            prompt = f'{t}<|speech_start|>{token}<|im_end|>'\n",
    "            prompt = f'{speaker}: {prompt}'\n",
    "            \n",
    "            outputs = tokenizer(prompt, add_special_tokens = False)\n",
    "            position = range(len(outputs['input_ids']))\n",
    "            length = len(outputs['input_ids'])\n",
    "            \n",
    "            if count + length > block_size:\n",
    "                o = collator(temp, position_ids)\n",
    "                out.write(o)\n",
    "                temp = [outputs['input_ids']]\n",
    "                position_ids = [position]\n",
    "                count = length\n",
    "                \n",
    "            else:\n",
    "                temp.append(outputs['input_ids'])\n",
    "                position_ids.append(range(len(outputs['input_ids'])))\n",
    "                count += len(outputs['input_ids'])\n",
    "        \n",
    "        if len(temp):\n",
    "            o = collator(temp, position_ids)\n",
    "            out.write(o)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "917e9a73-f9b0-444d-9ef9-9ad36dc2d88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 1335.74it/s]\n"
     ]
    }
   ],
   "source": [
    "loop((data[:100], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02c22462-f2d0-446f-9b06-8bd8ad7a4d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LocalDataset('tokenized-4k-qwen3-chunk/tokenized-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88b0fbcf-ee80-4988-9103-8d20bfeb54b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2014337"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d87ec286-9379-483f-b897-56161197e32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      " 15%|█▌        | 15345/100000 [00:09<00:54, 1543.60it/s]=(true | false)\n",
      "100%|██████████| 100000/100000 [01:05<00:00, 1535.22it/s]\n",
      "100%|██████████| 100000/100000 [01:07<00:00, 1487.93it/s]\n",
      "100%|██████████| 100000/100000 [01:08<00:00, 1460.97it/s]\n",
      "100%|██████████| 100000/100000 [01:09<00:00, 1431.36it/s]\n",
      "100%|██████████| 100000/100000 [01:09<00:00, 1431.28it/s]\n",
      "100%|██████████| 14337/14337 [00:10<00:00, 1391.04it/s]]\n",
      "100%|██████████| 100000/100000 [01:11<00:00, 1407.02it/s]\n",
      "100%|██████████| 100000/100000 [01:11<00:00, 1403.86it/s]\n",
      "100%|██████████| 100000/100000 [01:11<00:00, 1395.67it/s]\n",
      "100%|██████████| 100000/100000 [01:13<00:00, 1359.96it/s]\n",
      "100%|██████████| 100000/100000 [01:12<00:00, 1386.10it/s]\n",
      "100%|██████████| 100000/100000 [01:13<00:00, 1365.05it/s]\n",
      "100%|██████████| 100000/100000 [01:12<00:00, 1374.98it/s]\n",
      "100%|██████████| 100000/100000 [01:13<00:00, 1367.61it/s]\n",
      "100%|██████████| 100000/100000 [01:13<00:00, 1366.85it/s]\n",
      "100%|██████████| 100000/100000 [01:13<00:00, 1364.92it/s]\n",
      "100%|██████████| 100000/100000 [01:14<00:00, 1350.69it/s]\n",
      "100%|██████████| 100000/100000 [01:13<00:00, 1352.69it/s]\n",
      "100%|██████████| 100000/100000 [01:13<00:00, 1358.95it/s]\n",
      "100%|██████████| 100000/100000 [01:13<00:00, 1351.71it/s]\n",
      "100%|██████████| 100000/100000 [01:13<00:00, 1368.07it/s]\n"
     ]
    }
   ],
   "source": [
    "from multiprocess import Pool\n",
    "\n",
    "chunks = chunks(data, 100000)\n",
    "pool = Pool(20)\n",
    "pooled = pool.map(loop, chunks)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4350b7d7-7a4e-4638-8cdb-c95b5e752507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "print('a')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
