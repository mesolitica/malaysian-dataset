{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80e5df2e-648c-4306-9e99-248151b6b2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from datasets import Audio\n",
    "from transformers import AutoTokenizer\n",
    "from streaming import MDSWriter\n",
    "from streaming.base.format.mds.encodings import Encoding, _encodings\n",
    "from streaming import LocalDataset\n",
    "import streaming\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import os\n",
    "import json\n",
    "from multiprocess import Pool\n",
    "import itertools\n",
    "\n",
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield (l[i: i + n], i // n)\n",
    "\n",
    "def multiprocessing(strings, function, cores=6, returned=True):\n",
    "    df_split = chunks(strings, len(strings) // cores)\n",
    "    pool = Pool(cores)\n",
    "    pooled = pool.map(function, df_split)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    if returned:\n",
    "        return list(itertools.chain(*pooled))\n",
    "\n",
    "class UInt32(Encoding):\n",
    "    def encode(self, obj) -> bytes:\n",
    "        return obj.tobytes()\n",
    "\n",
    "    def decode(self, data: bytes):\n",
    "        return np.frombuffer(data, np.uint32)\n",
    "\n",
    "_encodings['uint32'] = UInt32\n",
    "\n",
    "columns = {\n",
    "    'input_ids': 'uint32',\n",
    "    'position_ids': 'uint32',\n",
    "    'attention_mask': 'uint32',\n",
    "    'audio': 'str',\n",
    "    'text': 'str'\n",
    "}\n",
    "hashes = 'sha1', 'xxh64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d4cba60-9d62-4cfb-9b72-489029790a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('accept-period.json') as fopen:\n",
    "    period = set(json.load(fopen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9feefbb9-0a84-470d-aaac-c1ff2e35d7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = pd.read_parquet('processed.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bc51da7-11da-43f4-937f-013d8d19f58b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reference_text              Uhm, hello, selamat pagi ye, saya dari custome...\n",
       "generate_text               Encik, bolehkah Encik memberikan maklum balas ...\n",
       "normalized_generate_text    Encik, bolehkah Encik memberikan maklum balas ...\n",
       "reference_audio                                          husein-assistant.mp3\n",
       "filename_audio                                   response-husein-v3/55420.mp3\n",
       "speaker                                                                husein\n",
       "similarity                                                           0.800952\n",
       "audio_length                                                         4.400181\n",
       "index                                                                 1367694\n",
       "alignment                   [{'end': 0.38, 'score': -3.89, 'start': 0.12, ...\n",
       "averaged_pitch              [279.365, 104.309, 97.318, 94.798, 95.233, 91....\n",
       "distances                   [0.073, 0.007, 0.012, 0.006, 0.007, 0.012, 0.0...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8cea3088-d7fc-4d7e-8563-32274aa41263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1645455/1645455 [01:22<00:00, 19934.81it/s]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "for i in tqdm(range(len(processed))):\n",
    "    if i not in period:\n",
    "        continue\n",
    "    index = processed.iloc[i]['index']\n",
    "    speaker = 'husein' if 'husein' in processed.iloc[i]['filename_audio'] else 'idayu'\n",
    "    data.append({\n",
    "        'speaker': speaker,\n",
    "        'text': processed.iloc[i]['normalized_generate_text'],\n",
    "        'token': f'distilcodec/{index}.json',\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ea8dde3c-554c-41fd-942d-eb986e362a5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "977091"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bfb8b178-3517-4c82-9723-26914bcffc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('mesolitica/Malaysian-TTS-1.7B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2a3a802e-3dbf-437c-8f4e-9431afef067e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def collator(batch, batch_position_ids):\n",
    "    input_ids = []\n",
    "    position_ids = []\n",
    "    masks = []\n",
    "    for i in range(len(batch)):\n",
    "        l = len(batch[i])\n",
    "        input_ids.extend(batch[i])\n",
    "        position_ids.extend(batch_position_ids[i])\n",
    "        masks.append(l)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': np.array(input_ids).astype(np.uint32),\n",
    "        'position_ids': np.array(position_ids).astype(np.uint32),\n",
    "        'attention_mask': np.array(masks).astype(np.uint32),\n",
    "        'audio': '',\n",
    "        'text': '',\n",
    "    }\n",
    "\n",
    "def slice_and_balance(nested_list, size):\n",
    "    first = []\n",
    "    balance = []\n",
    "    current_size = 0\n",
    "\n",
    "    for sublist in nested_list:\n",
    "        if current_size < size:\n",
    "            remaining_space = size - current_size\n",
    "            if len(sublist) <= remaining_space:\n",
    "                first.append(sublist)\n",
    "                current_size += len(sublist)\n",
    "            else:\n",
    "                first.append(sublist[:remaining_space])\n",
    "                balance.append(sublist[remaining_space:])\n",
    "                current_size = size\n",
    "        else:\n",
    "            balance.append(sublist)\n",
    "    \n",
    "    return first, balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "068f9036-5ca5-4fc3-ab73-cbba6df21fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf tokenized-4k-qwen3\n",
    "!mkdir tokenized-4k-qwen3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "68fa1ae0-3d75-4a5f-8195-f4613abc84ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "sequence_length = 4096\n",
    "def loop(files, block_size = sequence_length):\n",
    "    rows, index = files\n",
    "    out_root = f'tokenized-4k-qwen3/tokenized-{index}'\n",
    "    os.system(f'rm -rf {out_root}')\n",
    "    count = 0\n",
    "    temp = []\n",
    "    position_ids = []\n",
    "    last_block, last_position_block = None, None\n",
    "    with MDSWriter(out=out_root, columns=columns, compression=None, hashes=hashes) as out:\n",
    "        for row in tqdm(rows):\n",
    "\n",
    "            speaker = row['speaker']\n",
    "            with open(row['token']) as fopen:\n",
    "                token = json.load(fopen)\n",
    "            token = ''.join([f'<|speech_{t}|>' for t in token])\n",
    "            t = row['text']\n",
    "            prompt = f'{t}<|speech_start|>{token}<|im_end|>'\n",
    "            prompt = f'{speaker}: {prompt}'\n",
    "            \n",
    "            outputs = tokenizer(prompt, add_special_tokens = False)\n",
    "            position = range(len(outputs['input_ids']))\n",
    "            length = len(outputs['input_ids'])\n",
    "            \n",
    "            if count + length > block_size:\n",
    "                o = collator(temp, position_ids)\n",
    "                out.write(o)\n",
    "                temp = [outputs['input_ids']]\n",
    "                position_ids = [position]\n",
    "                count = length\n",
    "                \n",
    "            else:\n",
    "                temp.append(outputs['input_ids'])\n",
    "                position_ids.append(range(len(outputs['input_ids'])))\n",
    "                count += len(outputs['input_ids'])\n",
    "        \n",
    "        if len(temp):\n",
    "            o = collator(temp, position_ids)\n",
    "            out.write(o)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c49e4216-1aa5-4c71-939a-ff3d92e68489",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 459.21it/s]\n"
     ]
    }
   ],
   "source": [
    "loop((data[:100], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b1b0189c-f7ad-45d8-b04b-8614826dcd55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = LocalDataset('tokenized-4k-qwen3/tokenized-0')\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "87d9039d-95f9-4de7-bd3a-701d3614a6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  4%|▎         | 701/20000 [00:01<00:36, 526.35it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  0%|          | 93/20000 [00:00<00:42, 465.66it/s]]]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  3%|▎         | 614/20000 [00:01<00:36, 524.84it/s]]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  7%|▋         | 1431/20000 [00:02<00:35, 518.52it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  7%|▋         | 1322/20000 [00:02<00:35, 523.03it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 471.83it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 471.72it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 468.76it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 469.10it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 466.19it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 467.60it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 468.85it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 469.34it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 467.51it/s]\n",
      "100%|██████████| 20000/20000 [00:43<00:00, 464.26it/s]\n",
      "100%|██████████| 20000/20000 [00:43<00:00, 460.79it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 470.77it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 466.88it/s]\n",
      "100%|██████████| 20000/20000 [00:41<00:00, 476.49it/s]\n",
      "100%|██████████| 20000/20000 [00:43<00:00, 463.76it/s]\n",
      "100%|██████████| 20000/20000 [00:43<00:00, 462.02it/s]\n",
      "100%|██████████| 20000/20000 [00:43<00:00, 456.95it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 466.08it/s]\n",
      "100%|██████████| 20000/20000 [00:44<00:00, 451.63it/s]\n",
      "100%|██████████| 20000/20000 [00:43<00:00, 462.34it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 470.31it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 467.18it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 466.78it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 469.93it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 468.66it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 467.76it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 465.96it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 472.67it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 465.28it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 468.47it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 465.78it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 472.45it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 470.37it/s]\n",
      "100%|██████████| 20000/20000 [00:43<00:00, 459.41it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 467.87it/s]\n",
      "100%|██████████| 20000/20000 [00:41<00:00, 476.22it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 468.27it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 467.12it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 469.31it/s]\n",
      "100%|██████████| 20000/20000 [00:44<00:00, 453.82it/s]\n",
      "100%|██████████| 17091/17091 [00:34<00:00, 490.33it/s]\n",
      "100%|██████████| 20000/20000 [00:41<00:00, 485.32it/s]\n",
      "100%|██████████| 20000/20000 [00:41<00:00, 485.30it/s]\n",
      "100%|██████████| 20000/20000 [00:41<00:00, 483.20it/s]\n",
      "100%|██████████| 20000/20000 [00:41<00:00, 481.29it/s]\n",
      "100%|██████████| 20000/20000 [00:41<00:00, 480.50it/s]\n",
      "100%|██████████| 20000/20000 [00:41<00:00, 485.25it/s]\n",
      "100%|██████████| 20000/20000 [00:41<00:00, 476.80it/s]\n",
      "100%|██████████| 20000/20000 [00:42<00:00, 472.54it/s]\n"
     ]
    }
   ],
   "source": [
    "from multiprocess import Pool\n",
    "\n",
    "chunks = chunks(data, 20000)\n",
    "pool = Pool(20)\n",
    "pooled = pool.map(loop, chunks)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d078a6cd-7d95-4b8b-ab02-78081b639afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = glob('tokenized-4k-qwen3-streaming/tokenized-*')\n",
    "folders.extend(glob('tokenized-4k-qwen3-chunk/tokenized-*'))\n",
    "folders.extend(glob('tokenized-4k-qwen3/tokenized-*'))\n",
    "folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c550bae7-f1e7-45e5-ba55-6f1aa363c4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf packing-qwen3-combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c98de0e-2701-4841-931f-ae4de697df55",
   "metadata": {},
   "outputs": [],
   "source": [
    "with MDSWriter(out='packing-qwen3-combine', columns=columns, compression=None, hashes=hashes) as out:\n",
    "    for f in folders:\n",
    "        try:\n",
    "            dataset = LocalDataset(local=f)\n",
    "            for i in tqdm(range(len(dataset))):\n",
    "                out.write(dataset[i])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
