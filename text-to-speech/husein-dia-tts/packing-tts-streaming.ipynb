{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "790f9c74-e0d2-49ee-b122-2f83b56176e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from datasets import Audio\n",
    "from transformers import AutoTokenizer\n",
    "from streaming import MDSWriter\n",
    "from streaming.base.format.mds.encodings import Encoding, _encodings\n",
    "from streaming import LocalDataset\n",
    "import streaming\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import os\n",
    "import json\n",
    "from multiprocess import Pool\n",
    "import itertools\n",
    "\n",
    "def chunks(l, n):\n",
    "    for i in range(0, len(l), n):\n",
    "        yield (l[i: i + n], i // n)\n",
    "\n",
    "def multiprocessing(strings, function, cores=6, returned=True):\n",
    "    df_split = chunks(strings, len(strings) // cores)\n",
    "    pool = Pool(cores)\n",
    "    pooled = pool.map(function, df_split)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    if returned:\n",
    "        return list(itertools.chain(*pooled))\n",
    "\n",
    "class UInt32(Encoding):\n",
    "    def encode(self, obj) -> bytes:\n",
    "        return obj.tobytes()\n",
    "\n",
    "    def decode(self, data: bytes):\n",
    "        return np.frombuffer(data, np.uint32)\n",
    "\n",
    "_encodings['uint32'] = UInt32\n",
    "\n",
    "columns = {\n",
    "    'input_ids': 'uint32',\n",
    "    'position_ids': 'uint32',\n",
    "    'attention_mask': 'uint32',\n",
    "    'audio': 'str',\n",
    "    'text': 'str'\n",
    "}\n",
    "hashes = 'sha1', 'xxh64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "730cf2f8-11ab-4d67-b8c2-6785938fca92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5327569/5327569 [01:11<00:00, 74898.87it/s]\n"
     ]
    }
   ],
   "source": [
    "df_mapping = pd.read_parquet('chunk-streaming-flatten.parquet')\n",
    "mapping = {}\n",
    "for i in tqdm(range(len(df_mapping))):\n",
    "    mapping[df_mapping.iloc[i]['filename_audio']] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b631fd8-98b2-4eb3-aebb-981551b10128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chunk': array([array(['Menurutnya, kejadian itu dipercayai',\n",
       "               'chunk-streaming/prepare-dataset-normalizer-text-malay-news-husein_41906_0.mp3'],\n",
       "              dtype=object)                                                                     ,\n",
       "        array(['berlaku',\n",
       "               'chunk-streaming/prepare-dataset-normalizer-text-malay-news-husein_41906_1.mp3'],\n",
       "              dtype=object)                                                                     ,\n",
       "        array(['di Kilometer tiga puluh empat lebuh raya berkenaan pada pukul enam titik sepuluh petang,',\n",
       "               'chunk-streaming/prepare-dataset-normalizer-text-malay-news-husein_41906_2.mp3'],\n",
       "              dtype=object)                                                                               ,\n",
       "        array(['kelmarin.',\n",
       "               'chunk-streaming/prepare-dataset-normalizer-text-malay-news-husein_41906_3.mp3'],\n",
       "              dtype=object)                                                                     ],\n",
       "       dtype=object)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet('chunk-streaming.parquet').to_dict(orient = 'records')\n",
    "df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d17e75ab-31f8-4674-8a09-a3d4c52660dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "977091"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('accept-streaming-chunk.json') as fopen:\n",
    "    accepted = set(json.load(fopen))\n",
    "\n",
    "df = [df[i] for i in range(len(df)) if i in accepted]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4241c27c-9253-4dc0-9fba-94d581e2d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('mesolitica/Malaysian-TTS-1.7B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80f1463a-d457-42dc-9d98-03c9d46ba045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "def collator(batch, batch_position_ids):\n",
    "    input_ids = []\n",
    "    position_ids = []\n",
    "    masks = []\n",
    "    for i in range(len(batch)):\n",
    "        l = len(batch[i])\n",
    "        input_ids.extend(batch[i])\n",
    "        position_ids.extend(batch_position_ids[i])\n",
    "        masks.append(l)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': np.array(input_ids).astype(np.uint32),\n",
    "        'position_ids': np.array(position_ids).astype(np.uint32),\n",
    "        'attention_mask': np.array(masks).astype(np.uint32),\n",
    "        'audio': '',\n",
    "        'text': '',\n",
    "    }\n",
    "\n",
    "def slice_and_balance(nested_list, size):\n",
    "    first = []\n",
    "    balance = []\n",
    "    current_size = 0\n",
    "\n",
    "    for sublist in nested_list:\n",
    "        if current_size < size:\n",
    "            remaining_space = size - current_size\n",
    "            if len(sublist) <= remaining_space:\n",
    "                first.append(sublist)\n",
    "                current_size += len(sublist)\n",
    "            else:\n",
    "                first.append(sublist[:remaining_space])\n",
    "                balance.append(sublist[remaining_space:])\n",
    "                current_size = size\n",
    "        else:\n",
    "            balance.append(sublist)\n",
    "    \n",
    "    return first, balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38b615b1-6e8e-4b42-b325-50f48e672825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf tokenized-4k-qwen3-streaming\n",
    "!mkdir tokenized-4k-qwen3-streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "30a0bec8-72d9-4141-8acf-b8bf58d49a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = df[0]['chunk']\n",
    "speaker = 'husein' if 'husein' in chunk[0][1] else 'idayu'\n",
    "prompt = ''\n",
    "for c in chunk:\n",
    "    with open(f'chunk-streaming-flatten/{mapping[c[1]]}.json') as fopen:\n",
    "        token = json.load(fopen)\n",
    "    token = ''.join([f'<|speech_{t}|>' for t in token])\n",
    "    prompt += f'{c[0]}<|speech_start|>{token}<|im_end|>'\n",
    "prompt = f'streaming,{speaker}: {prompt}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "817e3706-2a96-489d-95e8-3eb6a2f4f8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "sequence_length = 4096\n",
    "def loop(files, block_size = sequence_length):\n",
    "    rows, index = files\n",
    "    out_root = f'tokenized-4k-qwen3-streaming/tokenized-{index}'\n",
    "    os.system(f'rm -rf {out_root}')\n",
    "    count = 0\n",
    "    temp = []\n",
    "    position_ids = []\n",
    "    last_block, last_position_block = None, None\n",
    "    with MDSWriter(out=out_root, columns=columns, compression=None, hashes=hashes) as out:\n",
    "        for row in tqdm(rows):\n",
    "\n",
    "            chunk = row['chunk']\n",
    "            speaker = 'husein' if 'husein' in row['chunk'][0][1] else 'idayu'\n",
    "            prompt = ''\n",
    "            for c in chunk:\n",
    "                with open(f'chunk-streaming-flatten/{mapping[c[1]]}.json') as fopen:\n",
    "                    token = json.load(fopen)\n",
    "                token = ''.join([f'<|speech_{t}|>' for t in token])\n",
    "                prompt += f'{c[0]}<|speech_start|>{token}<|im_end|>'\n",
    "            prompt = f'streaming,{speaker}: {prompt}'\n",
    "            \n",
    "            outputs = tokenizer(prompt, add_special_tokens = False)\n",
    "            position = range(len(outputs['input_ids']))\n",
    "            length = len(outputs['input_ids'])\n",
    "            \n",
    "            if count + length > block_size:\n",
    "                o = collator(temp, position_ids)\n",
    "                out.write(o)\n",
    "                temp = [outputs['input_ids']]\n",
    "                position_ids = [position]\n",
    "                count = length\n",
    "                \n",
    "            else:\n",
    "                temp.append(outputs['input_ids'])\n",
    "                position_ids.append(range(len(outputs['input_ids'])))\n",
    "                count += len(outputs['input_ids'])\n",
    "        \n",
    "        if len(temp):\n",
    "            o = collator(temp, position_ids)\n",
    "            out.write(o)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be06ef06-13d1-4d26-9e3e-7a43ebc0ee53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 585.21it/s]\n"
     ]
    }
   ],
   "source": [
    "loop((df[:100], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4954c0de-372f-49fe-b9b9-08f679cda350",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "  7%|▋         | 1417/20000 [00:02<00:31, 587.19it/s]\n",
      "100%|██████████| 20000/20000 [00:35<00:00, 557.97it/s]\n",
      "100%|██████████| 20000/20000 [00:35<00:00, 560.40it/s]\n",
      "100%|██████████| 20000/20000 [00:35<00:00, 565.05it/s]\n",
      "100%|██████████| 20000/20000 [00:35<00:00, 558.74it/s]\n",
      "100%|██████████| 20000/20000 [00:35<00:00, 558.05it/s]\n",
      "100%|██████████| 20000/20000 [00:36<00:00, 553.98it/s]\n",
      "100%|██████████| 20000/20000 [00:35<00:00, 561.87it/s]\n",
      "100%|██████████| 20000/20000 [00:36<00:00, 555.34it/s]\n",
      "100%|██████████| 20000/20000 [00:35<00:00, 558.64it/s]\n",
      "100%|██████████| 20000/20000 [00:35<00:00, 556.47it/s]\n",
      "100%|██████████| 20000/20000 [00:35<00:00, 558.94it/s]\n",
      "100%|██████████| 20000/20000 [00:35<00:00, 560.69it/s]\n",
      "100%|██████████| 20000/20000 [00:36<00:00, 547.88it/s]\n",
      "100%|██████████| 20000/20000 [00:35<00:00, 562.54it/s]\n",
      "100%|██████████| 20000/20000 [00:35<00:00, 567.06it/s]\n",
      "100%|██████████| 20000/20000 [00:35<00:00, 558.08it/s]\n",
      "100%|██████████| 20000/20000 [00:35<00:00, 556.20it/s]\n",
      "100%|██████████| 20000/20000 [00:36<00:00, 554.85it/s]\n",
      "100%|██████████| 20000/20000 [00:35<00:00, 555.86it/s]\n",
      "100%|██████████| 20000/20000 [00:34<00:00, 574.41it/s]\n",
      "100%|██████████| 20000/20000 [00:34<00:00, 574.02it/s]\n",
      "100%|██████████| 20000/20000 [00:34<00:00, 581.40it/s]\n",
      "100%|██████████| 20000/20000 [00:34<00:00, 575.33it/s]\n",
      "100%|██████████| 20000/20000 [00:34<00:00, 579.33it/s]\n",
      "100%|██████████| 20000/20000 [00:34<00:00, 578.31it/s]\n",
      "100%|██████████| 20000/20000 [00:34<00:00, 584.66it/s]\n",
      "100%|██████████| 20000/20000 [00:34<00:00, 583.89it/s]\n",
      "100%|██████████| 20000/20000 [00:34<00:00, 576.84it/s]\n",
      "100%|██████████| 20000/20000 [00:34<00:00, 579.56it/s]\n",
      "100%|██████████| 20000/20000 [00:35<00:00, 562.56it/s]\n",
      "100%|██████████| 20000/20000 [00:34<00:00, 576.45it/s]\n",
      "100%|██████████| 20000/20000 [00:34<00:00, 580.46it/s]\n",
      "100%|██████████| 20000/20000 [00:34<00:00, 573.36it/s]\n",
      "100%|██████████| 20000/20000 [00:34<00:00, 581.72it/s]\n",
      "100%|██████████| 20000/20000 [00:34<00:00, 583.24it/s]\n",
      "100%|██████████| 20000/20000 [00:34<00:00, 580.97it/s]\n",
      "100%|██████████| 20000/20000 [00:34<00:00, 584.55it/s]\n",
      "100%|██████████| 20000/20000 [00:34<00:00, 582.23it/s]\n",
      "100%|██████████| 20000/20000 [00:34<00:00, 584.71it/s]\n",
      "100%|██████████| 20000/20000 [00:34<00:00, 584.70it/s]\n",
      " 39%|███▉      | 7764/20000 [00:13<00:21, 578.50it/s]]"
     ]
    }
   ],
   "source": [
    "from multiprocess import Pool\n",
    "\n",
    "chunks = chunks(df, 20000)\n",
    "pool = Pool(20)\n",
    "pooled = pool.map(loop, chunks)\n",
    "pool.close()\n",
    "pool.join()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
