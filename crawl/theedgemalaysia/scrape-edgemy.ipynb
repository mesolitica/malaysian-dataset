{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Scraping **The Edge Malaysia**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For [The Edge Malaysia](https://theedgemalaysia.com/), each of their articles seem to have a unique ID, e.g., \"https://theedgemalaysia.com/node/677590\". Hence, since we won't be able to do this by month, page no., etc., we'll use a **brute force** approach that tests every combination of numbers, such that we'll only scrape from a valid url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Upgrade-Insecure-Requests\": \"1\",\n",
    "        \"Sec-Fetch-Dest\": \"document\",\n",
    "        \"Sec-Fetch-Mode\": \"navigate\",\n",
    "        \"Sec-Fetch-Site\": \"none\",\n",
    "        \"Sec-Fetch-User\": \"?1\",\n",
    "        \"Cache-Control\": \"max-age=0\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get text and metadata from url\n",
    "def process_url(x):\n",
    "    while True:\n",
    "        webpage = f'https://theedgemalaysia.com/node/{x}'\n",
    "        try:\n",
    "            r = requests.get(webpage, headers=headers)\n",
    "            break\n",
    "        except:\n",
    "            time.sleep(5.0)\n",
    "    \n",
    "    soup = BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "    try:\n",
    "        headline = soup.find('title').text\n",
    "        h = soup.find('div', class_=\"news-detail_newsTextDataWrap__PkAu5\") \n",
    "        content_list = [p.text for p in h.find_all(\"p\")]\n",
    "        content_str = ' '.join(content_list)\n",
    "        if 'English version' in content_str:\n",
    "            language = 'Mandarin'\n",
    "        else:\n",
    "            language = 'English'\n",
    "\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    data = {'url': f'https://theedgemalaysia.com/node/{x}', \n",
    "            'headline': headline,\n",
    "            'language': language,\n",
    "            'content': content_str}\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initially I only wanted to scrape a subset of the website.\n",
    "I decided to proceed with the rest after batch 1 - 7.\n",
    "\n",
    "Pertaining to below, the range I initially specified neglects numbers like\n",
    "200000, 300000, 400000, etc. I've fixed this batch8 onwards.\n",
    "\"\"\"\n",
    "np.random.seed(10082023)\n",
    "batch1 = list(set([str(x) for x in np.random.randint(0, 100000, size=20000)]))\n",
    "batch2 = list(set([str(x) for x in np.random.randint(100001, 200000, size=20000)]))\n",
    "batch3 = list(set([str(x) for x in np.random.randint(200001, 300000, size=20000)]))\n",
    "batch4 = list(set([str(x) for x in np.random.randint(300001, 400000, size=20000)]))\n",
    "batch5 = list(set([str(x) for x in np.random.randint(400001, 500000, size=20000)]))\n",
    "batch6 = list(set([str(x) for x in np.random.randint(500001, 600000, size=20000)]))\n",
    "batch7 = list(set([str(x) for x in np.random.randint(600001, 700000, size=20000)]))\n",
    "\n",
    "batch8 = list(\n",
    "    set([str(x) for x in np.arange(500000, 600000)]) - set(batch6)\n",
    ")\n",
    "\n",
    "batch9 = list(\n",
    "    set([str(x) for x in np.arange(400000, 500000)]) - set(batch5)\n",
    ")\n",
    "\n",
    "batch10 = list(\n",
    "    set([str(x) for x in np.arange(300000, 400000)]) - set(batch4)\n",
    ")\n",
    "\n",
    "batch11 = list(\n",
    "    set([str(x) for x in np.arange(200000, 300000)]) - set(batch3)\n",
    ")\n",
    "\n",
    "batch12 = list(\n",
    "    set([str(x) for x in np.arange(100000, 200000)]) - set(batch2)\n",
    ")\n",
    "\n",
    "batch13 = list(\n",
    "    set([str(x) for x in np.arange(0, 100000)]) - set(batch1)\n",
    ")\n",
    "\n",
    "batch14 = list(\n",
    "    set([str(x) for x in np.arange(600000, 700000)]) - set(batch7)\n",
    ")\n",
    "\n",
    "batches = [batch1, batch2, batch3,\n",
    "           batch4, batch5, batch6,\n",
    "           batch7, batch8, batch9,\n",
    "           batch10, batch11, batch12,\n",
    "           batch13, batch14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consolidate links and get texts\n",
    "max_workers = 10\n",
    "\n",
    "for i, urls in enumerate(batches):\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(process_url, x) for x in urls]\n",
    "\n",
    "        for future in tqdm(futures, total=len(urls)):\n",
    "            result = future.result()\n",
    "            if result:\n",
    "                with open(f'theedgemalaysia--complete-batch-{i+1}.jsonl', 'a') as f:\n",
    "                    json.dump(result, f)\n",
    "                    f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Final checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_check = [\n",
    "    'theedgemalaysia--complete-batch-1.jsonl',\n",
    "    'theedgemalaysia--complete-batch-2.jsonl',\n",
    "    'theedgemalaysia--complete-batch-3.jsonl',\n",
    "    'theedgemalaysia--complete-batch-4.jsonl',\n",
    "    'theedgemalaysia--complete-batch-5.jsonl',\n",
    "    'theedgemalaysia--complete-batch-6.jsonl',\n",
    "    'theedgemalaysia--complete-batch-7.jsonl',\n",
    "    'theedgemalaysia--complete-batch-8.jsonl',\n",
    "    'theedgemalaysia--complete-batch-9.jsonl',\n",
    "    'theedgemalaysia--complete-batch-10.jsonl',\n",
    "    'theedgemalaysia--complete-batch-11.jsonl',\n",
    "    'theedgemalaysia--complete-batch-12.jsonl',\n",
    "    'theedgemalaysia--complete-batch-13.jsonl',\n",
    "    'theedgemalaysia--complete-batch-14.jsonl',\n",
    "    ]\n",
    "\n",
    "dfs = []\n",
    "for i, file in enumerate(files_to_check):\n",
    "    get_df = pd.read_json(file, lines=True)\n",
    "    get_df.drop_duplicates(inplace=True)\n",
    "    dfs.append(get_df)\n",
    "    print(f'No. of articles in batch {i+0}: {len(get_df)}')\n",
    "\n",
    "final_num_articles = pd.concat(dfs, axis=0)\n",
    "final_num_articles.to_parquet('theedgemalaysia--complete-1-14.parquet', index=False)\n",
    "print(f'Num. of articles/webpages scraped: {len(final_num_articles)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
