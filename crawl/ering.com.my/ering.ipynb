{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "from urllib.request import urlopen\n",
    "import urllib.request\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = [\n",
    "\"https://aliffchannel.com/netizen/\",\n",
    "\"https://aliffchannel.com/travel/\",\n",
    "\"https://aliffchannel.com/teknologi/\",\n",
    "\"https://aliffchannel.com/lifestyle/\",\n",
    "\"https://ering.com.my/category/islamik/\"]\n",
    "\n",
    "hrefs = []\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/56.0.2924.76 Safari/537.36'} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl(url):\n",
    "    while True:\n",
    "        try:\n",
    "            r = requests.get(url,headers = headers)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(1.0)\n",
    "\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    div = soup.find_all('h2', attrs = {\"class\":\"title\"})\n",
    "    \n",
    "    if len(div) == 0:\n",
    "        return\n",
    "\n",
    "    for link in soup.find_all('h2', attrs = {\"class\":\"title\"}):\n",
    "\n",
    "        href = link.find('a').get('href')\n",
    "\n",
    "        hrefs.append(href)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_worker = 20\n",
    "\n",
    "\n",
    "for url in links:\n",
    "    \n",
    "    print(url)\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "    a = soup.find_all('a', attrs = {\"class\":\"page-numbers\"})\n",
    "    a = [a_.get('href') for a_ in a if a_.get('href')]\n",
    "    pgno = [a_ for a_ in a if 'page' in a_]\n",
    "    max_page = max([int(a_.split('page/')[1].replace('/','')) for a_ in pgno])\n",
    "    print(max_page)\n",
    "\n",
    "    \n",
    "    for i in tqdm(range(1, max_page + 1, max_worker)):\n",
    "        aranged = np.arange(i, i + max_worker)\n",
    "        urls = [f'{url}page/{a}' for a in aranged]\n",
    "        with ThreadPoolExecutor(max_workers=max_worker) as executor:\n",
    "            futures = {executor.submit(crawl, url): url for url in urls}\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            future.result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrefs = list(set(hrefs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10235"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hrefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'ering-link.json', 'a') as f:\n",
    "    json.dump(hrefs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ering-link.json') as fopen:\n",
    "    hrefs = json.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_url(x):\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            r = requests.get(x, headers=headers)\n",
    "            \n",
    "            if r.status_code == 508:\n",
    "                time.sleep(20)\n",
    "                continue\n",
    "                \n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            time.sleep(5.0)\n",
    "    \n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    try:\n",
    "        headline = soup.find('h1', class_=\"single-post-title\").text\n",
    "        h = soup.find('div', class_=\"entry-content clearfix single-post-content\")\n",
    "        p = [p.text for p in h.find_all(\"p\") if p.text and len(p.text.split()) > 5]\n",
    "    except Exception as e:\n",
    "        print('error in link:'+ x)\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "    \n",
    "    data = {'url': x, 'headline': headline, 'content': p}\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 7824/10235 [43:24<08:13,  4.89it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTPSConnectionPool(host='ering.com.my', port=443): Max retries exceeded with url: /2019/08/promosi-merdeka-oppo-tawar-rebat-menarik-untuk-warga-emas-bermula-sekarang-1-september-2019/ (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x1544d6a40>, 'Connection to ering.com.my timed out. (connect timeout=None)'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10235/10235 [56:18<00:00,  3.03it/s] \n"
     ]
    }
   ],
   "source": [
    "max_workers = 10\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    futures = [executor.submit(process_url, x) for x in hrefs]\n",
    "    \n",
    "    for future in tqdm(futures, total=len(hrefs)):\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            with open('ering-2.jsonl', 'a') as f:\n",
    "                json.dump(result, f)\n",
    "                f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
